<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />    
	<script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="./css/styles.css" />
    <script src="./js/common.js"></script>
    <script>      
    </script>
    <title>ICDAR 2023 Competition on Language-Guided Document Editing (DocEdit)</title>
  </head>
  <body>
    <div class="h-screen" style="overflow-x: hidden">
      <!-- Header -->
      <div id="header">
		<!-- relative h-screen-1/3 -->
        <div style="height: 80px;">
		  <!-- bg-transparent -->
          <nav id="navbar" class="border-gray-200 px-2 sm:px-4 py-2.5 rounded bg-white">
            <div class="container flex flex-wrap justify-center items-center mx-auto" >              
			  <div style="padding-left: 50px;padding-right: 50px;">
				<!--<img src="./img/icdar2023_logo_light_horizontal.png" style="height: 50px;"> -->
				<img src="./img/draft_logo_light_horizontal_chart_only.png" style="height: 50px;">				
			  </div>
              <div class="hidden w-full md:block md:w-auto" id="mobile-menu">
                <ul
                  class="flex flex-col mt-4 md:flex-row md:space-x-8 md:mt-0 md:text-sm md:font-medium"
                >
                  <li>
                    <a
                      href="./index.html"
                      class="block hover:bg-stone-700 hover:text-white text-white bg-stone-700 px-3 py-2 w-full rounded-md text-md font-semibold"
                      >Home</a
                    >
                  </li>
                  <li>
                    <a
                      href="./tasks.html"
                      class="block text-dark hover:bg-stone-700 hover:text-white px-3 py-2 rounded-md text-md font-semibold"
                      >Tasks</a
                    >
                  </li>
                  <li>
                    <a
                      href="./schedule.html"
                      class="block text-dark hover:bg-stone-700 hover:text-white px-3 py-2 rounded-md text-md font-semibold"
                      >Schedule</a
                    >
                  </li>
				  <li>
                    <a
                      href="./registration.html"
                      class="block text-dark hover:bg-stone-700 hover:text-white px-3 py-2 rounded-md text-md font-semibold"
                      >Registration</a
                    >
                  </li>
                  <li>
                    <a
                      href="./toolsanddata.html"
                      class="block text-dark hover:bg-stone-700 hover:text-white px-3 py-2 rounded-md text-md font-semibold"
                      >Tools and Data</a
                    >
                  </li>
                  <li>
                    <a
                      href="./contactus.html"
                      class="block text-dark hover:bg-stone-700 hover:text-white px-3 py-2 rounded-md text-md font-semibold"
                      >Contact Us</a
                    >
                  </li>
                </ul>
              </div>
            </div>
          </nav>          
        </div>
      </div>

      <!-- Home's Content -->
      <div class="container mx-auto px-4 text-justify">
        <div class="text-center">
          <h1 class="font-medium leading-tight text-3xl my-4 text-gray-600">
            <b>ICDAR 2023</b> <b>C</b>Competition on <b>L</b>anguage-<b>G<b/>uided <b>D</b>ocument <b>E</b>diting <br/>
		(DocEdit)
          </h1>
        </div>
        <div class="flex flex-col md:flex-row mb-4">
          <div class="flex flex-col w-full md:w-1/2 mx-2">
            <h2 class="font-medium leading-tight text-2xl my-4 text-gray-600">
              Summary
            </h2>
            <p>
              This competition is composed of two subtasks 
			  related to language-guided document editing. The first subtask is <b>Edit Command Generation</b>,  
			  The second subtask is <b>Edit Localization</b> 			  
			  Entrants in the competition may choose to
              participate in any number of sub-tasks, which are evaluated in
              isolation, such that solving previous sub-tasks is not necessary.
              We hope that such decomposition of the problem of automated document editing will draw broad participation from the Document
              Analysis and Recognition (DAR) community.               
            </p>
          </div>
          <div class="flex flex-col w-full md:w-1/2 mx-2">
            <h2 class="font-medium leading-tight text-2xl my-4 text-gray-600">
              Competition Updates
            </h2>
                    <div style="height: 180px; overflow-y: scroll;">
						<table style="margin-top:1px" cellspacing="10px">
							<!--
							<tr> <th align="left"> Deadline </th> <th align="left"> Event </th> </tr>
							-->
						  <!--
						  <tr>
							<td style="text-align:top; background-color:brown; color:white">   
							<b>&nbsp Nov 3 &nbsp</b>   
							</td> 
							<td> <a href="/toolsanddata.html" class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600">Training & Testing</a> sets are publicly available. </td>
						  </tr>
			  
						  <tr>
							<td style="text-align:top; background-color:lightsalmon; color:white">   
							<b>&nbsp May 23 &nbsp</b>   
							</td> 
							<td> <a href="/leaderboards.html" class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600">Leaderboards</a> is out. </td>
						  </tr>

						  <tr>
							<td style="text-align:top; background-color:lightsalmon; color:white">   
							<b>&nbsp May 9 &nbsp</b>   
							</td> 
							<td> Testing dataset is released. </td>
						  </tr>
						-->
						  <!--
							<tr>
								<td style="text-align:top; background-color:lightsalmon; color:white">   
								<b>&nbsp Feb 23 &nbsp</b>   
								</td> 
								<td> Training dataset is released. <a href="https://www.dropbox.com/s/85yfkigo5916xk1/ICPR2022_CHARTINFO_UB_PMC_TRAIN_v1.0.zip?dl=0" class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600">Download</a></td> 
							</tr>
						-->
							<tr>
								<td style="text-align:top; background-color:rgb(48,60,72); color:white">   
								<b>&nbsp Dec 30, 2022 &nbsp</b>   
								</td> 
								<td> <a href="./registration.html" class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600">Registration</a> is now open!</td> 
							</tr>								
							<tr> 
								<td style="text-align:top; background-color:rgb(48,60,72); color:white">   
									<b>&nbsp Dec 29, 2022 &nbsp </b>   
								</td>
								 
							</tr>
						</table>
					</div>
          </div>
        </div>
        <div class="flex flex-col justify-center">
          <img
            src="./img/icdar2023_tasks.png"
            alt="tasks"
            class="md:max-w-3xl self-center"
          />
        </div>
        <div class="flex flex-col mb-4">
          <h2 class="font-medium leading-tight text-2xl my-4 text-gray-600">
            Background
          </h2>
          <p>
            Digital documents are used extensively to help people improve business productivity (drafting contract agreements,
presentations decks, letterheads, invoices, resumes, form filling) and communicate with customers through online advertisements, social media posts, flyers, posters, billboards,
web and mobile app prototypes, etc. However, modern document editing tools require a skilled professional to work on a
large screen. Challenges emerge when complex editing
operations require multiple different functionalities wrapped
within the editing tools for text and image region placement,
grouping, spatial alignment, replacement, resizing, splitting,
merging and special effects. As the creation and editing of documents becomes more ubiquitous and increasingly used by
novice users on mobile devices, there is an increasing need to improve the accessibility of these tools through an intelligent assistant system that can understand user’s intent and
translate it into executable code that can be processed by the
editing tool to fulfilling a user’s editing needs.
          </p>
          <div class="flex flex-col justify-center text-center my-4">
            <img
              src="./img/charts.png"
              alt="charts"
              class="md:max-w-4xl self-center"
            />
            <p>
              Example chart types to use in the proposed competition. Note that
              all these were generated from the same tabular data.
            </p>
          </div>
          <p>
            The DAR community has displayed a continued interest in classifying
            types of charts as well as processing charts to extract data. In the
            past decade, multiple applications have been built around automatic
            processing of charts such as retrieval, textual summarization of
            charts, making charts more accessible on the web, automatically
            redesigning charts, automatically assessing chart quality,
            preservation of charts from historical documents, chart data
            plagiarism detection, bibliometrics, visual question answering and
            accelerating discovery of new materials.
          </p>
          <!-- <p class="my-4">
            Prior competitions related to this area include DeTEXT which
            concentrated on detecting text in figures and ImageCLEF competition
            on medical compound figure separation and multi-label
            classification. However, these competitions do not concentrate on
            end-to-end data extraction from scientific charts, but include these
            as a sub-type of scientific figures in general.
          </p> -->
        </div>

        <div class="flex flex-col mb-4">
          <h2 class="font-medium leading-tight text-3xl my-4 text-gray-600">
            Competition Outline
          </h2>
          <h3 class="font-medium leading-tight text-2xl my-4 text-gray-600">
            Dataset
          </h3>
          <p>
            Our previous competitions used both real and synthetic charts datasets for all tasks. For ICDAR 2023, we are providing a <b>extended UB-UNITEC PMC dataset</b>, which contains real charts extracted from Open-Access publications found in the <a
              class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600"
              href="https://www.ncbi.nlm.nih.gov/pmc/"
            >
              PubMedCentral.
            </a>(<b>PMC</b>). Following the same protocol, we only picked images released under a Creative Commons By Attribution license (CC-BY), which allows us to redistribute them to the competition participants. 
          </p>
          <br/>
		  <p>
		  For this competition, <b>UB-UNITEC PMC</b> has a new, larger training set (by merging training and testing sets from previous competition), and a novel testing set that is also collected and annotated on real charts. Participants can use additional datasets (e.g. <b>Adobe Synth</b> from previous competition) to improve the performance of their framework, as long as it is notified in their final result.
		  </p>
		  
          <h3 class="font-medium leading-tight text-2xl my-4 text-gray-600">
            Evaluation
          </h3>
          <p>
            On March, 2023, we will release the test datasets, and by
            <b>March 15th, 2023</b> competition participants are expected submit the
            following:
          </p>
          <ol class="list-decimal list-inside">
            <li>Predictions on each test dataset</li>
            <li>A short but complete system description</li>
          </ol>
          <br />
          <p>
            The organizers will tabulate the results for each task and present it at
            <a
              class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600"
              href="https://www.icdar2023.org/"
            >
              ICDAR 2023 in San Jose, California, USA
            </a>.
            Note that you do not need to attend ICDAR 2023 to participate in this
            competition.
          </p>
          <br />
		  <p>The tasks and sub-tasks considered in this competition are</p>
		  <ol class="list-decimal list-inside">
			<li>
				<b>Task 1:</b> Stepwise Data Extraction
				<ol class="pl-5 list-decimal list-inside">
					<li><b>Sub-task 1.1:</b> Chart Image Classification (e.g. bar, box, line)</li>
					<li><b>Sub-task 1.2:</b> Text Detection and Recognition</li>
					<li><b>Sub-task 1.3:</b> Text Role Classification (e.g. title, x-axis label)</li>
					<li><b>Sub-task 1.4:</b> Axis Analysis</li>
					<li><b>Sub-task 1.5:</b> Legend Analysis</li>
					<li>
					  <b>Sub-task 1.6:</b> Data Extraction
					  <ol class="list-alpha list-inside ml-8">
						<li><b>Sub-task 1.6.a:</b> Plot Element Detection/Classification</li>
						<li><b>Sub-task 1.6.b:</b> Raw Data Extraction</li>
					  </ol>
					</li>
				</ol>
			</li>
			<li><b>Task 2:</b> End-to-end Data Extraction</li>
			<li><b>Task 3:</b> Visual Question Answering</li>
		  </ol>
          
          <br />
          <p>
            Each subtask is evaluated in isolation, meaning that systems have
            access to the ideal output (i.e. Ground Truth) of previous subtasks.
            Subtasks 1.3-1.5 are considered parallel tasks and only receive the
            outputs of subtasks 1.1 and 1.2. For example, the input for the Axis
            Analysis sub-task is the chart image, chart type, text bounding
            boxes, and text transcriptions. Each sub-task has its own evaluaton
            metric, detailed on the <a href="./tasks.html" class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600">Tasks</a> page.
          </p>
          <br />
          <p>
            Participants are not obligated to perform all subtasks 
			and may submit test results for any set of subtasks 
			they wish. Methods may be submitted for the
            end-to-end data extraction task where systems are only given the chart
            image (no intermediate inputs) and are expected to produce the raw
            data used to create the chart (same output as subtask 1.6.b).
			Additionally, methods may be submitted for the visual question answering
			task where system also only given the chart image and a question about 
			the chart, and they are expected to produce the right answer based
			on the chart.
          </p>
        </div>
        <div class="flex flex-col mb-4">
          <h2 class="font-medium leading-tight text-2xl my-4 text-gray-600">
            Registration
          </h2>
          <p>
		    Register <a href="./registration.html" class="underline text-blue-600 hover:text-blue-800 visited:text-purple-600">here</a>.
          </p>
        </div>
        <div class="flex flex-col mb-4">
          <h2 class="font-medium leading-tight text-2xl my-4 text-gray-600">
            Acknowledgements
          </h2>
          <p>
            The creation of our manually curated real CHART dataset was
            partially supported by the National Science Foundation under Grant
            No.1640867 (OAC/DMR).
          </p>
        </div>
      </div>
    </div>
  </body>
</html>
